{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## GPT 2 Fine-tuned Model Story Generation"
      ],
      "metadata": {
        "id": "Ze8eEtJoQ8ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last modified: 06-01-2024\n",
        "\n",
        "Name: Mahamuge Dinendra Nadishan Costa\n",
        "\n",
        "UWE Student Number: 13030224"
      ],
      "metadata": {
        "id": "1QGPssVK2vbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the required library updates for both accelerate and transformers\n",
        "#from huggingface\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "\n",
        "# This resets the environment automatically to apply the installations above.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "D1VpeGY64zR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0668357d-eeef-44f9-c7b4-e5d05a5d6d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/265.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/265.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.25.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wymba1qWOpa9",
        "outputId": "6fbd047b-f628-40ef-e750-a1472d565067"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the device to CUDA (or GPU) for processing\n",
        "import torch\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "print(f'Using Device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkLJiI6Z43EN",
        "outputId": "4f0d949a-c891-459a-80fa-98d00048ccdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the required GPT2 modules from the transformers library for running\n",
        "#GPT2 model\n",
        "from transformers import GPT2Tokenizer,GPT2LMHeadModel,TrainingArguments,Trainer,DataCollatorWithPadding\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "# If a local copy of the fine-tuned model is unavailable, use the following two lines to load the model and tokenizer to load it directly from Huggingface:\n",
        "\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "\n",
        "\n",
        "# Run the following to load the fine-tuned model on stories with original lengths\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT 2 Finetuned/tokenizer\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT 2 Finetuned/model\")\n",
        "\n",
        "\n",
        "# Run following two lines to load the fine tuned model trained on equivalent length stories\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT 2 Finetuned with data balance/tokenizer\")\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT 2 Finetuned with data balance/model\")\n",
        "\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "#Send the model to process within the GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "jWua4ZciFQMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e069b3f-894f-422d-e74b-8db23e117c47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the fine-tuned model with 10 prompts"
      ],
      "metadata": {
        "id": "V5XRuOSkyZcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the model to evaluate to run prompts on the model for generation\n",
        "model.eval()\n",
        "\n",
        "#Set prompt text for model\n",
        "prompt_text = \"He drew his sword and pointed at the peculiar\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c20472-e4f0-44f6-c653-d17a24067b56",
        "id": "dbFxstTi02-A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He drew his sword and pointed at the peculiar creature, whose white, rounded face was hidden beneath two large bushy eyebrows and a mouth adorned by small teeth set in a line down the middle of its face—its jaw itself, a very unusual feature in these lower vertebrates, and which was adorned by two large, round teeth, one set in front and one set behind on both of its heads, and which were adorned with a yellow, fleshy band at the top and sides, a ring of bright, violet-hued feathers, a ring of silver on each side of its body, a fin exactly like that of the skiff of an eel or the argonauts, and various other appendages from the tail, wings, teeth, and so forth—all objects highly suggestive of some superhuman being, though none of them had the ring of a demon, and none were even faintly resembling the wings or the tail of a human being—and, finally, drew forth his crony of a brother, whom he met on the platform, not as the messenger of the sun or moon, but holding in one hand the weapon of a policeman and in the other the whip of a wild bull, and was about to lash the creature with the whip of the same name, when the thing which had been so unusual to him suddenly changed into such a state of sluggish animation that it seemed to him like an actual apparition, for the whip had come out just at this moment, and the appearance of that whip had left him dangling and unable to move a step or draw back a step amid the hectic scene that had passed near and was now hanging over him like a specter on a date-palmer, while his eyes followed the movements of its bearer with hazy gleaming irises, as though they followed them with a sort of dazzle and fascination, and his whole being seemed concentrated and drawn in one direction only from the moment he had drawn it until it was out of his reach, when the whip had disappeared and he stood still, his arms extended, his body leaning almost unconsciously, his sword bent at a point over his head, his sword between his legs like a club held out to the wind, and as he held his sword with one hand, all the while cursing the fate which had befallen him, he felt an unpleasant, uncomfortable sensation about his loins just above his waist, and at the same instant the prodigious energy of his sword came back again and he slashed at the thing with the whip as on his shoulder and as suddenly as it had vanished, with a speed and precision that startled even himself, a blow which sent the thing sprawling with a ringing crash in the air, and the next thing he knew it had passed and was gone! “I shall never see such a creature again! But at least I will always see an impertinent creature, and one which shall always be threatening to those who meet her.’s approach; this is a menace to me, and I give you the whip, for if I am taken, I declare myself at your service forever,” and with that voice of mildness and humanity which the younger generation of the Musketeers had always impressed on him, Conseil added, “You can give me no rest till I surrender my weapon.”; but to this effect, with a movement of the head which showed that his sword was lodged between his two hands and not in any part of his body, he raised his head and spat a bitter, insolent, spiteful word at him, which Conseil understood but half as well as I did, and in spite of his stifled protestations he drew the whip from his belt and held it about his throat—a position which was almost imperceptible even to his master and quite out of his reach, but which seemed to have a dreadful impact upon the thing with which he had been about to fight and to produce a terrible effect upon me as he thrust it at this creature who had now changed into such a perfectly harmless and complaisant personage on the platform of the Reform Club, a creature which was evidently much agitated and much impressed by the changes which came upon him in this world in the shape of this transformed creature, who was now turned into the hero of that world, with his sword raised and his voice hoarse and hoarse and hoarse, as if he were going to deliver his final message to his persecutors—though I could hear no more as the hour drew on and the noise of voices increased—though, on the contrary, everything seemed to be going according to plan and things seemed at last to settle down in the tranquil and peaceful life that my companions and I had led so happily and securely for a year and a half, in which we had just had a simple dinner, a fruitless exercise in which we had done without any food for a long time, and, besides, nothing to amuse ourselves with except playing cards and whistling and jingling at random.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the model to evaluate to run prompts on the model for generation\n",
        "model.eval()\n",
        "\n",
        "prompt_text = '''Nihara the warrior princess was calm in the face of danger.\n",
        "No one dared challenge her now.\n",
        "'''\n",
        "\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WnaAreSubBg",
        "outputId": "28ee29b3-1ff0-4eb2-8dda-85d419fa2834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nihara the warrior princess was calm in the face of danger.\n",
            "No one dared challenge her now.\n",
            "“It will be easy for you, my lord! You will have the upper hand for some time, and nobody will want to draw swords against you when you have such formidable opponents!” It was not difficult to see what effect her courage had produced in this eccentric woman, who in the meantime, had begun to grow wise to a real art, and could not but perceive that she had won the hearts of her friends by a strange drama, when the old man’s words, full of gallantry, made her utter surprise and terror on seeing the effects of her audacity upon the young Musketeer, who, not only looked her fair in the face, but smiled on seeing the joy which was at his birth, and the gratitude she felt toward the great lord who inspired in her heart by having made her father, a handsome knight, and who was no doubt, at the time, one of the most illustrious men in Japan, the most exalted in his kingdom, and lord of a kingdom which, since the fall of the Japanese Empire, had always held out to him with a lofty and proud reputation—lord of Japan, lord general of Japan, and lord of the whole Asia, where, after the war, his domains were extended to every point except Edo; lord of the East, where his territories were extended to every part except Kyushu; lord of Nagasaki, lord of Chikina, lord of Nagasaki, and lord of Kyushu—a kingdom which, according to official sources, extended from Edo to Nagasaki, extending from Japan to Nagasaki, in truth, an immense kingdom in the shape of a single state, a vast state with an area of 370 square leagues, hence of 700 leagues! The Japanese, then, declare that this is the only true and true Japanese _enjoy_ of the Nautilus! The Nautilus had crossed the whole Indian peninsula, not only the East, which islets line the coasts of China and America and Japan and even the Laccadive, from the coast of India down to the coasts of America! It had traveled from one end of the globe to the other, from Cape of Good Hope to Cape of Foxtrot, but had never entered the sea between the Tuamotu and the Tuamotu or the Cape of Bestia; the sea being at that point an incalculable wilderness! It had passed through several seas in a row, and in the midst of these waves of water had come to the islands of Java, located on the south-west coast of the island group of the Malay Archipelago, off Sumatra proper (whose name, appropriately enough, the Arabic language pronounced “Tuamotu,”), and upon the northeast coast of Sumatra proper, off the coast of Malaysia, a small island in the heart of an immense territory whose surface area measures 4,000 leagues long and five leagues wide! It was on its voyage from this coast to the coast of Ceylon, and from Ceylon to Java, with the whole gamut of local customs and local traditions! It would have gone, then, even to the New World, to that continent where so many thousands of sailors and so many ships had perished in the Pacific during the war: the Spanish-American conflict, the French-Ned Land voyagers of 1857, the Portuguese-Bartolomeo, the Portuguese-Guadalupe, finally the Frenchman-Frederick de Meneses in his ill-fated attempt in 1866, the English-Barry Whiting in 1867, the navigator-Frederick Bowdoin in 1873; to the island of Tahiti—an island whose existence was questioned even then by the celebrated navigator Commander Cook, chief of the Bureau of Oceanography at the Paris government, who, for some years afterwards, had insisted that its true nature be recognized—to that island, which the Japanese have always denied ever having existed, as I have already stated! To that island, which had been mentioned to Captain Dumont d’Urville as having sunk during the Battle of Neufchâtel, the Englishman, in 1794, and in 1867, when that island was still unknown to all but its most fervent believers! But it was then that this whole bizarre narrative began to seem to unravel itself! The story began to decline into a kind of inevitable monologue, only to recede again like the drops of a river; it grew darker and darker until, in essence, the sea became a battlefield in which I could hear cries and groans escaping from a thousand mouths! As I heard this narrative receding from me I came to perceive that it had become an apoplectic fit, like those fits in the antechamber.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the model to evaluate to run prompts on the model for generation\n",
        "model.eval()\n",
        "\n",
        "prompt_text = \"And so it was the beginning of a beautiful friendship. Perhaps even something more. \"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "QlDLmZFWug1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429bb20d-17b4-4776-8242-3218379607d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And so it was the beginning of a beautiful friendship. Perhaps even something more.     Pshaw! What can he be thinking, when we are so near a decision as to go with him to the ‘Nam?’? Not a thing, not a thought! You see now, my young friend, what a long, winding road it is to decide; and when you reach a decision, you can take a hundred opposite ways, and it is not probable you are ever going to reach a decision again; and you might as well adopt my plan as mine to-morrow, and tomorrow it would be so much wiser, because your friends will be as good or as bad as they are or are; so you have got to be patient!” “Do you believe so?” cried Andrew Stuart; and his eyes flashed round for a moment for a moment into icy fire, and then fell without another instant in the depths of his heart, as one resigned himself to wait till they were sure they were no more likely than not to be at all sorry for having to deal with a person who, for an instant, he felt as good as if his life depended upon him, and whom a minute before they had been so glad to have avoided! “I tell you,” returned Andrew Stuart, in a soft voice, as he walked up and down again on his horse, and spoke with a tone of deep conviction, “I do not believe a word you are saying,” as he passed his arm through his friend’s; “and so I can only hope, in time, to find you at last, for if I am wrong, my dear fellow, and he kills you after the fashion in which you treated him, and you kill me a second time as he threatened, there is still time! You see now, Andrew Stuart, I will save you from myself; believe me, I am not so far from it as you believe, and if you are so far, I will make you a fortune, if you will but wait till I have saved you!--and so much the better!--and then God will reward you! But of what price could I stake my life in you, now, now, now, dear fellow, now that you are as hot-headed as you were yesterday? I should only save myself with the hope of someday making so great a profit over the life of that poor woman as you sell her for! But where would I get the hundred guineas, now that I was about to stake my wife?” Andrew Stuart stopped again, and looked as if he were in a daze; and at the top of his head a long, blackish shadow, almost like smoke, which descended almost to the ground, grew brighter and vanished, and was followed by a red, glowing glow of an altogether different colour and a intensity that did not correspond with the brightness of the sun, until it made a circle round it and vanished in a red-tinged smoke which glided and rolled through the air like smoke from the burning pine trees, leaving only a red, warm suggestion in the air and a trailing, faintly perceptible vapour behind it, at the edge of the hot air in which the conversation was going on in the train, until it disappeared completely concealed us in the crowd and swept us across the vast empty space which was between the station and the house of the log-house on the slope of Putney Hill in one of the pleasant, leafy suburbs which the railway extended slowly southward till it merged with the clear, blue air on the hill side and gave us a view of Maybury to the east and west, before we reached the great Putney valley, which was still grey and grey upon the horizon against the blue sky, and seemed suddenly to grow red again through the grey branches and bright green woods to the westward of the station, and then changed to a bright, blue colour again upon the slope of Charing Cross and Kingston upon the hill, till at last it was nothing but the blackness before us, before we left Putney again and reached Kingston on the broad, clear Sunday before the sodden dawn of the first Monday in the Month of September? Then, with a long, hollow cry, like the thud of an echo, the wheels of the Martian came trotted swiftly along upon the empty pavement of Maybury, rolling with the waves until the wheels bumped against one another, as if there were no ground upon which to dig the pit and lay their victim! At last, just as the wheels had settled down upon the ground, the wheels sounded again and again with a continuous succession of growls and growls, but this time accompanied with the distinct, piercing thud of an enormous foot--a foot like the tail of a great tiger--which crawled slowly through the heather on the side of the road, creeping towards us, until it was concealed among the heather by the thick, still heather on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the model to evaluate to run prompts on the model for generation\n",
        "model.eval()\n",
        "\n",
        "prompt_text = \"Thine eyes shine like bright diamonds\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD5E6kiNud-1",
        "outputId": "39364446-0ac6-4534-a908-bf7b3ee60d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thine eyes shine like bright diamonds; Thy hair is golden; thy tongue doth quench the blood which doth quench thine open wounds.—Thou art the Prince of Verona, which hath slain Romeo, and Romeo died.—The Prince of Verona shall live; but thou shalt not have power to slay the King of Scotland, Who was thy father, and hadst thou been but half of his height, Thou shouldst have killed him, and taken his crown, I should know it’d I should kill thee.— Romeo,” said she, in anger, “remember how it is that in thy life thou play’st, and when thou hast done, It is like a wretch that lives but a month longer.—Why, thou do’t forget.—Thou wouldst slay me.— I will not forget.— And if thou hadst but one wish, That I could bestow on thee, It would be my death, for it were the happiest of all That thou mayst have.—If thou hadst but one thing, and that was death, thou wouldst deny It to any but my cousin.— But now I’ll speak one thing, and I would tell thee that love, Hath but one name, and that is sisterhood.— Why, what is it, I know not how to name.—What love, ho! If thou hadst not one, I could not name it To thee: and if it were that thou wouldst, I could not devise A better.— What love, how then? If love were death, would that I should not have thee, If I knew it?—What love, how then? If I should be bound To call thee love, well—why, why, this is cruel! Thou dost deny’st me love, why, I deny it.— O, what love, how then? Romeo, cousin! Where is your cousin? Hath he not found thee? Why, home again! O, that thou mayst not find me.—Come away, I say.—O, this is cruel! How then? Come away and let me speak it To you.—What love would I have, had I but that!” And he went down from the box and set himself again Upon the bench; for he felt that the time Was coming when he had to speak to a lover, And yet he was ignorant what kind of a lover that was, Either prince or gentleman.—Then could he make no reply, To make love himself.—Then could he not endure Being spoken to by the very man who should have Confided To him the secret of Romeo, When this very man was at the top of love? And at this very minute The Prince of Verona himself was at the apex of that love Which he had sworn.—Then might he not endure To speak either to Romeo or to any other, Yet he was at the apex of all love, Whom he owed everything to and could not own Desire without Desire, and Desire without love: and it became him To live and to die by the very root of that root, Whom this very root had already choked.—And what then? Might not this root be choked, And that root choked himself?—what then? Would not this gallant knight, this brave gentleman, this true gentleman be choked, Too proud of his own virtues, And live and die by the very root, from which this root had choked all ambition? And yet, perhaps, might he not feel that this root was indeed choked, And from which he might perhaps die? But what if he felt at once that this root was choked? Might he not feel that his very self was choked likewise, By this root which was choked from him by that root? Was he not choked by love too? The great cause was the root of ambition; and if there were any love That was great and powerful, this love might be choked From the fountain whence it had flowed through all the veins Of human nature, for it was its very root! Now was it fit that every man should feel the power of death Upon this root, that root that fountain, by that root, by that root, And not be able to move it from its foundation? Then did the great love spring to a head, And then did it spring to a blow, That all men should know, By that root which he himself held in his power, this same root that fountain did drink To all the blood which flowed to him.—O, how now, that we know not what it is to die! Come away, I pray thee!—O, how soon.—What need we now, that we know nothing at all—I have done nothing, and death is the beginning of it.—What haste! what haste! I am slain! Come away, it be done, death is the end of me! Come, be gone! The time is near! The hour soon near! The third part of it Is near! the hour near!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the model to evaluate to run prompts on the model for generation\n",
        "model.eval()\n",
        "\n",
        "prompt_text = \"Fly you fools!\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "2cMPL-ASugmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29d11de-af7a-41fc-af2a-2d9cef1415bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fly you fools! I will have you arrested, I do entreat you! “ “ I pardon you,” cried Buckingham; “ pardon you, then, my dear love, pardon you, and I do love you!” “Be satisfied; you will see to it that everything is at the least possible; I am not at all guilty of your anger, although I know what is in your heart.” “How, you know what?” “Oh, I—I know how your love comes from you; I know all that is in the heart of a lion; I know what your grief is—even what you would not dare to tell me! You know—” “Yes; but you know also that I love you, since you never forget what I have told you—even when I send you my last words, I love you, since I love you, and yet you forget! You know, my dear Constance! Let it suffice, then; you shall love me as I love you.” “I will not lose my time in thinking how you would feel when my executioner came for you, you know; at least I shall not forget you—for as soon as he appears in the world you will remember me.” “But you, what would you say to an executioner of the Guards—one of my lieutenants, a man in a robe like that which I put on at five minutes before the siren!” “He will kill him like a madman!” cried the cardinal, who could scarcely restrain himself from reviving with joy the feelings which had so often animated the young man—an executioner!” The young woman became still more pale as if on receiving this reproach, and breathed more sharply; she could not speak, and in a tremble she stood fixed upon his feet, with her arms crossed in the hope of stifling him with a blow from the scimitar, and in a few seconds she had fallen down on one knee, crying out, “Hold!” This last cry confused the executioner; he raised her by clasping her in his arms, but with a terrible gesture he disengaged his crutch from her hands, holding out Milady’s head so far that the blow broke it off, and a fresh blow from the executioner darted into the chamber opposite, and shattered the window, which fell with a crash in the midst of the fragments of broken glass, and was then hung with a tattered lace upon the rail of the door leading into the apartment; and this proved that the door was open, although on the contrary there stood guards armed with machetes and daggers, and that a man could pass through there without being seen by anyone but the two women who guarded the door—those who, to see Milady, were themselves seen, or rather heard, passing in a corridor through which they usually went to meet the young woman on entering from behind the gate of the Louvre, although they stood there by the gate of the Louvre, on horseback, or by one of the windows of the Luxembourg; and on either hand they concealed their horses from anyone who approached the carriage at the door of the Louvre but Milady herself, unless it were a cardinal and his wife, in whose presence she appeared to feel more at home than if she were at home among the hôtel of Athos or Porthos or Aramis, or D’Artagnan, or among the palanquins of the Duc de Chaulnes or of the Guards or Dessessart or of the Duc d’Angoulême by the gate of Amiens or Fogg.’ The executioner, who had not yet recovered from the terror which this recital occasioned in the eyes of the terrified young woman, did not know what to say to her, except, “Let the executioner go, madame, and be satisfied.” At this injunction Milady, who had been stupefied, drew back her head to conceal the sound of her voice which had so changed its depth, and was only now able, with a terror she knew not how, to utter those words which made the bed roll under her and make her cry out: “Ah!” And then, yielding to the persuasion which Kitty entertained that she was the cause of the change in the disposition of the queen’s men, she uttered a profound sigh; but her voice was still lower than when she uttered that cry, and the trembling of her limbs, which Kitty said she was undergoing at the bottom of her heart, was audible in her heart, like that of the dying victim of the arms of a demon who is pierced so deep in the heart that he is incapable of uttering a single syllable, was mute and mute amid the tumult of thoughts which Kitty uttered without even the effort of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the model to evaluate to run prompts on the model for generation\n",
        "model.eval()\n",
        "\n",
        "prompt_text = \"The underground passage led to heaven\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 2.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "vaOosc_WugZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c76a49-fffd-4eb3-a169-3c4c90173302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The underground passage led to heaven and back again, to the world of the living and the dead, and to the world of the angels, to the world of the devils, to the world of the animals, to the world of the ants and the flies, to the world of the tiny creatures, to the world of the ants and the flies, and so forth; to the whole infinite vastness of space, and back again and again to the world of the living and the dead, to the earth as a whole, to the sun and the moon, to the whole vastness of time, and back to the world of the ants and the flies! It was the most wonderful paradox, the most wonderful paradox in the whole Book of Ecclesiastes! It was a paradox, a paradox so profound and profound that for a time the mind of the young man was darkened by it, and he sank deeper and deeper into reverie, while the beautiful woman sat in the armchair by the fire and listened to this profound reverie with her eyes shut and her hands stretched out upon the table; and at length he began to think that the whole world was but a dream in the ear, a dream that could be broken at any instant, by a drop of water or by a single word, by the slightest suggestion from his mistress or by the slightest sound in the direction of his chamber or of his mistress’s chamber! And as he dreamt this, he was filled with doubt and perplexity, and he began to turn away from the house and towards the church, towards the little chamber in which she stood, and every moment to think and to wonder at himself, and each moment to wonder at the others, and yet the more he thought the more he was convinced of the truth of what he was hearing and the reality of what he saw and the reality of what he heard, and each moment to turn round, to look at the clock, to turn his back upon the table, to turn his ear to the clock, to turn his nose to the clock, to turn his stomach to the miller’s ale, to return to bed with the thought that he must be dreaming or with the thought that he must be hearing something—at last he was so convinced that he fell into a heavy slumber, fell into a profound slumber, and slept no more! And as he slept, each minute he heard a fresh noise in the air, and each minute the door of his chamber opened and closed, and an angel appeared, and as soon as the door had closed he heard a voice crying:  ‘Who is there? Who is it?  Enter, young man!  Enter, young man!’ At length he heard the sound of a shutter, and when he opened it the face of the young woman was in front of him, but it was not Aramis, but it was the beautiful young lady whom he had seen in the armchair who was singing away at the window, and who, according to his fancy, had come to carry him away with her, as a messenger from the cardinal or as a messenger from the emissary of his Eminence! The young woman was not Aramis, she was the angel who had come to carry him away, she was the Comtesse de Wardes, the Comtesse de Longueville, the Comtesse de la Fère, the Comtesse de la Trémouille, the Comtesse de la Trémouille! Aramis was dead; the Comtesse de Wardes had married another wife, the Comtesse de la Trémouille, the Comtesse de la Fère, the Comtesse de Longueville, the Comtesse de la Trémouille! “What, young man!” said the beautiful young lady, as she opened the shutter of the chamber in which she stood, “what, young man?” “Who is that whom you have just seen, young lady?” “Oh, no, no, no, no!” said the young lady; “that is my Lord Bonacieux, the Comtesse de Wardes, the Comtesse de Longueville, the Comtesse de la Trémouille!” “The Comtesse de Longueville!” cried Aramis, with astonishment; “that Comtesse de Longueville!” “Yes, yes, that Comtesse de Longueville!” “He has been arrested!” “Oh, no, it is he!” “No, it is he!” “Oh, no, it is he!” “Yes, that is the Comtesse de Longueville!” “Yes, that is the Comtesse de Longueville!” “Yes,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt_text = \"The ray gun vaporized the entire town under the sea\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "8jsVLyp5ugLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a550f48-4cb8-4643-ac20-7ffe7bfeb829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ray gun vaporized the entire town under the sea’s intense rays, leaving little in its way but smoky black ruins that I couldn’t linger in; the smoke obscured the moon’s light, hid the horizon with an atmospheric haze, and made our way through a carpet of crimson and crimson gas that drifted over the waves to form a brilliant halo over the waters, lighting up my way like an illuminator hung with threads of incandescent light! Before long, I could see every street, every low street corner, and every house on every street and in every castle on every hill, even the little ones with their low entryways, their few verandas, their modest lawns gleaming in the sunlight! For a while I stood amazed at that dazzling effect, marveling at its power, when, half-fascinated, I saw only a dark shadowed horizon stretching between the towers of one of the town’s tallest buildings, its black spire, with its minarets and gables; a mere fringe of sky above its parapet of red roofs that masked the full extent of its gleam and hid my brother’s hôtel from view; and in the upper stories of those buildings, not a soul seemed to have moved—not a sign of life on the terrace, not a sound but the distant click of hoofs, the rustle of a breeze! But gradually, the atmosphere began to clear, and the view of the sea revived; the horizon opened up before our eyes, until it appeared to be strewn with black specks, mingling with the undulating shadows of distant hills that darted over the surface of the waves to form strange bands of smoke rising above the trees in the distance; and then came a brief twilight, a moment that proves the power of the element over the human mind! I could now see the little houses along the terraces rising up, their clapboard and bustle slackening, their open plan windows giving forth the light of the fresh spring breeze; then the dimness fell once more, as if the world had suddenly become dark; the streets crept back into being, and the houses seemed again to be alive, even though I stood in the midst of the eternal darkness that had swallowed me whole—the darkness of the dead! The panels in my study swung wide open, and sunlight streamed through them, streaming down upon me, as it seemed, with irresistible heat, as though I were burning inside the panels! Afterward I could see the stars again, for I now saw the heavens from the windows of some of those little houses that had sprung up against the sky; they glittered in their new setting and seemed alive and vivid in their strange light, like some living things—live little stars, that I might say, under the sea’s intense glow! I went to bed and slept like a man at rest, contented with tossing about the bed in slumbering bewilderment, but awaking in bed at precisely seven o’clock in the morning with a start and a start of pain! Half-fascinated, I went down to the breakfast room to breakfast; but there I encountered, with a sinking feeling of dread, a man in a black doublet, clad in a dark coat, and face as black as pitch, as though he had drunk some poisonous liquor of doubtful quality! Meanwhile the crew were busy with preparations for departure, when I heard the door open suddenly, and a man in a dirty blue frock enter the lounge and join us! “Captain, here you are!” I said to the man, who was on my arm, leaning on a bench in the lounge, who, on the word of a sailor, had just made himself quite an imposing figure by leaping about among the sailors and making himself into a stir among them; and he, too, was talking excitedly with another sailor, who, thanks to his musket, was making good his appearance under the stern window blind, and had evidently come down on this occasion with his sights set on me! I put down the glass and the breakfast in my arms, and fell into the arms of the coxswain, whose legs were tired, and without saying a word to him, we took to the sea together! Meanwhile the crew hurried off to a safer place; I, who was sure they were taking us there in order to finish off the last pirates I had seen! “Is it possible this fellow is the same as the one who shot my poor little brother?” I muttered; and then, seeing the mutineer on board, with his eyes shining like the starlight from the port window, I whispered to the doctor that he was in the hands of an Englishman of fine repute who wanted to take me prisoner, for England is a lawless land.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt_text = \"Kushan, Nadishan and Dilshani were primordial entities that defended the very fabric of reality\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "2sKrOpttuf8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992b2249-2312-403c-f81f-51aa92aaf4c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kushan, Nadishan and Dilshani were primordial entities that defended the very fabric of reality with an indomitable vigor unrivaled by any of the primordial races on this earth—evidently, the giants, the monkeys, the sheep, the ants, the flies, and reptiles of the earth! What an extraordinary vigor in defending itself against the invaders of space that, at the end of one month, the whole earth had yielded! During the last three months of March, no less than ten large ships had encountered this prodigious narwhale: the China, the Russia, the Helvetia, the CASSANDRA, the Mauritius, the Mauritius IIII, the Java, and finally, that mysterious frigate, the Abraham Lincoln! Ten vessels? Impossible! The Abraham Lincoln, commander of, I fear, the most prodigious whaling vessel afloat in the world, had only to deal with one whale every three months! But this prodigious narwhale had increased in speed since it left the seas of Japan almost a year ago! During that same period of time, the seas of China and India had yielded ten large ships, not one of them had encountered this prodigious monster! It was now the summer solstice of the Chinese seas; it was also the equinox of India, as the Hindu calendar puts it, because, on March 21, the siberian moon would disappear for some days, then reappear the following day, giving the new year a little less than six months for rest until the new year, in the month of March, the Chinese calendar, in other words, the most solemn month for the Hindu religion! Six months! Ten months—they were running short on time! The Abraham Lincoln, then, had to maneuver around the most arduous tract of ice in the world: a tract stretching from the Bay of Bengal to the Bay of Vigo, by virtue of the narwhale’s speed! What was the point of such a voyage if the narwhale would not stop there, and if the Abraham Lincoln did not take the Suez Canal as a base, as it appeared to me, as a stepping stone for reaching the American coasts? On this very day, I would see the great ironclads, the biggest steamboats in the world, the biggest battleships, the biggest coal barges plowing the depths of the seas! By any other name, this voyage might have been called a foolish folly! But it was something much more, and far more noble—perhaps even life-changing, as I came to understand, as a voyage carried out with the utmost safety and without ever crossing a territory whose gates were impassable! Ten months gone by, and I was on board of the Abraham Lincoln just as if I had been jogged on the Ganges or the Volga, or had crossed the Bay of Bengal itself! Ten months gone by and I was on board of the Abraham Lincoln just as if I had been jogged along the Bundelkund, or the Ganges of the East, or along the Indus, the sacred waterways that separate the Hindu religion from the Mohammedans! Ten months gone by, and this American whaling vessel was waiting for me at its dock! Ten months gone by, since I had come into this world, when, as I went along the Indian and Oriental Railroad, one of my chief concerns was to reach Bombay—the eternal Indian capital! Ten months gone by, and this American whaling craft was dashing on the surface of the waves right in the center of this sacred sea! I had arrived there in fine time, and had scarcely tapped into this marvelous continent, if one might venture to speak of it! Ten months gone by, and if I could reach Bombay without being interrupted by a violent collision, what would become of me in the event of an accident on the way? Ten months gone by, and what would become of the thousands of refugees who fled from Bengal, or who settled in New York, after the closing of the Malabar Tunnel and the New York Central railway? Ten months gone by, and I, thrusting my way over a mountain pass so steep that the wheels seemed to grind to a halt! Ten months gone by! During the whole of my imprisonment in this prison I had spent at least twenty-five hours a day in this solitary place, in a solitary cell in the bow of this Nautilus! If, during my imprisonment, the interior of this prison had been as well ventilated as that of the outside, I had ample time for reading and writing; but it was not so, and such time was sacrificed continuously by the enormous capacity of this underwater prison! So that I was left to myself every twenty-five minutes, without a single thought! Meanwhile, I mustered all courage, and I resolved to continue our excursion into the heart of the ocean, which lies between latitude 50 degrees and 50 degrees south.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt_text = '''Nihara, Suj and Janu wielded the powers of the kingdom. They were the fairest and most beautiful princesses ever seen. '''\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 3.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHlOu82Vufup",
        "outputId": "68f75111-074c-4ac2-9b03-59f3ccd46436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nihara, Suj and Janu wielded the powers of the kingdom. They were the fairest and most beautiful princesses ever seen.  [Author’s Note: “The Japanese use the word “karawada, or “karanawada.” It can also be read as “girlage,” and as a translation of the Greek word, which means “crowned,” or “noble princess.”]” “Then,” said the judge, “if the three of them will come to the castle today, I wish them my daughter, my precious child, and they shall inherit my kingdom and all its treasures, as you wish.” The three of them bowed prodigiously, and the judge, without giving his reasons, dismissed them all with an air of serenity—a sentiment which reassured and comforted the three girls more than the sentence they had proposed to give in order to win their freedom—and went away to lead the search of the princesses after their new-found benefactress and her new-found benefactress’s child! On the fifth day the three young fellows were gone—sent by the kingdom to pursue the three enchantresses; but as soon as noon sounded, their tails came back with the scent of the evil spirit and their search was renewed, and by sundown they found the three of them in the forest, sitting together at one of the six tables in the castle reception-room, where the tables were arranged in the shape of a round house in shadow; and as before, they had brought in their noses with their smelling salts, which had revealed to their three companions the horrible secret of the princesses! “That’s well,” murmured an old woman, reclining upon a stool and shaking her head, “the princesses will be caught, and if we let their scent out they will devour the kingdom! I warn you, they will not escape.” “Oh!” “If we do not put up a fight,” replied the men of the wood, who were accustomed to do so, “I swear they will devour us.” The three of them began to cry out, for they had been warned, and yet they had done no such thing! Meanwhile, the three enchantresses who had been found were making themselves at home in the castle, and all was going as they wished, as they thought; so that they did not see who was coming or going, but only felt that it came from the king’s court, which was no great harm, as nobody was coming in but the king himself and nobody was going out but one of those three lackeys who accompanied him—that was the case in all other parts of Japan—but when the three of them were seized and carried off by the Japanese, all the attention fell on their three lackeys; and nobody dared to think of the three enchantresses and their little children in the reception-room, for the princesses alone were worth the trouble, and for them, if caught, their lives were forfeit to the world! As for me, my only fear was to hold on tight by the neck, so that my hair would always be caught in their web, or to let it hang down from the hooks in the passage behind me; but fortunately, I had managed to pull it up and put it about my neck through the meshes of the tapestry, where they hung down from the ceiling, in the shape of little red crescent shapes, instead of the round ones they were always accustomed to make when they went to get dressed in the morning; and now I was dressed as a lady of rank, and with my slippers on, and a little white cap on, I was just ready to go about and see my work at large at once! I sat down and fell asleep almost as I did before when I saw them; only, instead of looking at me as I slept or as I slept, they looked at me through the panels of their little bedroom windows, through the glass cases, which kept all the treasures of Japan hidden from the eyes of anybody who looked on them, and in short, through those little circular openings on the door of every window where everybody went to look on the sea! This was the dream of the three studsmen, who were always on the watch for new discoveries, such as the Nautilus had discovered in its first voyage round the world; and I went about my work at leisure as if nothing was amiss; and in the evening the princesses were gone and the three huntsmen were talking together in the castle ballroom—all the things that made my heart sing in the days of old—and then I felt myself free again, satisfied and at peace as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt_text = 'Aragon and Frodo along with Passepartout leapt from the plane'\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 6.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HZxX7yrufNN",
        "outputId": "3ddd27f1-c886-44ab-a924-9b17fae9eaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aragon and Frodo along with Passepartout leapt from the plane and came down a short, light, dusty street, before which stood a pavilion bearing an arborized arched entry like those used at Paris or Amiens, but with an inscription ARCHEMITH, as on a church rather than an office stand; for at the crossroads ran a large door of a curious species of arbor mold that was evidently at first a door in wooden construction; the mold of the windows around it glittered with diamonds, though the light was a pale blue color and disappeared in a pale twilight into the blackness of night, with a brightness less intense than that of daylight within a chamber suspended above a depth of dim darkness; and to the right an immense door of an unknown mold, though open to anybody who sought it with the least task, let them pass before the pavilion they sought, turning a corner of this immense gateway into the light by means of an immense lantern from which their passage was illuminated from two lower stories—above by another window, descending into the rooms in front, and between them and the church by an avenue which flowed through each story through a vast shade of green—with a last glow of the deepest blue at the bottom of an ascending stair of polished brickwork as it descended the main staircase to the first story in arbor—begging some light upon the strange aspect of that doorway, and on the street below—a side alley so shady it almost smote the heart of a traveler, but at its end a light was shone upon with singular distinctity through a large, soft, ebony scullery door, and standing before that door ran a bronze clock which, at a touch of the finger of the hand of the Mayor, threw forth a slow, pleasant, and faintly electric lamp, glowing, as he passed, pale pink, and without any personal charm or appertainment, a thin, rosy ray that sank into the darkness of evening, cool and almost glowing among the broken bricks that lay before it: then again it turned dark again, the glow dying away, until at length the last gleam went out and the bronze door slowly closed over the youth again, hanging, like an old arbor behind an office stand once more on the building within a broad shadow, against the dawn which presently streamed into that little garden of which it is still an observer, and upon Benares itself above all the city above it, with an inward glow to the east through a window which never opens again in its life and with that last glow of dismay in the heart of antechamber when a traveller thinks of the robber and of an intrigue that is at length ancillary to that return at the last step to reality that dwells downstairs in a chamber that looks to the world with the same appearance but lives and moves at Benares like an an empty shell above the clouds! For in that last moment all that darkness had closed round him as it had sealed all his fears—the doors of hell, the old darkness which shut out all light—until he saw once more a vast empty garden of a dark and mysterious kind, whither no shadow, no breath was permitted to lodge, a wide space of shadow—a garden that closed over every life, but the sonorous voice of children crying, like that from the chorus at the chorus on some hillside or some mountain side or on the flat terrace where nature is hidden and nature at once sits, an echo only now and again a bellowing echo of the echoes of the world; then, passing along the front and the south, he noticed that at length there ran a light under the gate, like a beam, shining pale and pink over every life which had lived here before, that light so faint as to fall slowly through every opening until it had lost its brightness like a beam of pure fire; then the light was almost extinguished, but here and there stood a pale, pale lamp, such as that which a blind man would cast, and there stood a small candle that was inanimate, casting dim, pallid shadows with the light of its rays; then he remembered a curious, quaint lamp in the gallery before his bed where a gentleman slept and a gentleman was at rest, but that lamp grew ever smaller and shorter—a lamp that, when the day was short and the evening light strong, grew weaker and duller; then he recalled an almost forgotten lamp which was beside the bed to the bed upon an unclean, smeared across the sheets, and which shone on a dusty staircase which lay between his chambers and the house upon which Eeyore was; now, upon a street to the south, a bright lamp glared faintly upon a little laboratory of a workshop, dim and brown but clear enough for the eyes to see where life breathed, but on a dark street at length a vast spiderweb suspended over life itself and bounding upon it as it went in order that it might run freely and softly through the court into the kitchen; now he remembered—on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt_text = \"Great Scott!\"\n",
        "\n",
        "#Convert the prompt text to tokens and add the attention mask\n",
        "input_ids = tokenizer(prompt_text, return_tensors = \"pt\").input_ids\n",
        "attention_mask = tokenizer(\n",
        "    prompt_text, return_tensors=\"pt\"\n",
        ").attention_mask\n",
        "\n",
        "#Send converted prompt and attention mask to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "\n",
        "#Generate the output from the model based on the parameter set here\n",
        "output = model.generate(\n",
        "    input_ids = input_ids,\n",
        "    attention_mask = attention_mask,\n",
        "    pad_token_id = tokenizer.pad_token_id,\n",
        "    max_length = 1024,\n",
        "    num_beams = 10,\n",
        "    min_length = 1000,\n",
        "    temperature = 6.5,\n",
        "    top_k = 50,\n",
        "    do_sample = True,\n",
        ")\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0],skip_special_tokens = True)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALUGRvbeqR5o",
        "outputId": "731f981f-1e52-4f9e-fd0a-e590126a256b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great Scott! If you can but walk, As soon shall we send you aboard With Gretel; she shall be better Nurse Than any else I have seen: and as you have a new tooth, She says your name first, and says that you have met with a villainous purpose: I give you the goose, and when you find She wants her, you can kill the grinder.’ “What villain has done this?” thought the queen, still in a great wonder; her heart beat So very heavy that her mouth quivered, For the sight of her grief might destroy her features like lightning: So, if she will but eat some root, Her hunger must cease: her mouth would seem like an empty jar, And her brain seem like a dead man; who will dare To think or act: what a misfortune it is By letting her live and thinking with her; what good can a king’s love bring In thinking or a love deed Have? She ate her spinach with her toast: but, when her fowl Lid fell upon golden brown, Her heart was too painful to draw forth any drop of water, Till on her golden cup there quivered to heaven Like a pot of poison gas, Against the whole earth whose living air She now had loathed so: how, that there might be some sound Of what was done without, when her royal love died, What voice had she, or of what event had brought So great an end to it all! And she let her head hang limp and drooped To the point of both senses: so that there might lie Some false sound upon her mouth to speak of events That were not, for fear, at all; that when she would have had a blow, That the royal love that she loved (to judge from her grief) Might have her say as with an empty cup; and with her head there, For pity’ll have filled her heart with fire: this was death to the eyes and hearing, Whom a true heart could not give up without pain.—God, well! what rage and ill rage she had!—she thought then of her cousin, For that he too had lost his heir: now grief was her only child, And therefore this death, when it seemed so much to make A man, would be made for her.—Then came another knocking.—Now had she brought home the best bonfire, That might not be turned upon her while she spoke,— But this came also knocking.—Then a raven cried, quite low in the night, With strange trumpeting and other odd cries, From out a low churchyard, crying up heaven to heaven, And bringing them in so low That at last heaven awoke, and laid hold on his hair, Shaking it against his face,—but he had nothing: “Why, what are you?”—the queen made him straight answer for his speech, By turning him back the third way, As if her hate and her pride Had laid him straight across the park, so that no sound of shoe, No voice, no way would let him up; only the most dreadful and loud roaring of voice Came from out the house itself.— “Are you come from Paris—is Paris coming on your heels, Or has this place grown bloody and dirty since yesterday?—behave well! you in peace; I have something more to tell To you; I dare not stand at the entrance of hell any longer, Unless in my power I can swear It is the will of goodness.—is this what you say, queen? This murder, it is surely, That hath struck you and your cousin this instant.’s death is doubtless the price of our peace To keep us here till we rid ourselves and find an end To love or war or beauty.— But she knew her own grief had grown worse, From the knowledge of Romeo till her death; now grief had spread to every part, Who had but a weak babe to bring her welcome, Who had no other tender or paternal care but to live: yet she thought, What a shame for a young man to dwell so long under such clouds, Being sure that the storm And the hurricane could scarce put out His father from him! but yet she let it go to every point, Which, if ever her grief got, it would return again.— How, how, he was going home again,—to all the kingdoms and nations, To every great and royal seat, Being sure no harm did him? yet he saw that they would gladly give him His royal coat, Whose motto would make us rich: but still, He could not move from the church seat, Where nothing but his royal pomp Could fill it but with such tears and cries: and the worst thing for a king to make himself stand In the court of King David was when he thought that every morning After his feast he took great care not to smile; what if it showed a frown? And that the grief Of the queen did all this, By thinking she would hide him; how should she\n"
          ]
        }
      ]
    }
  ]
}