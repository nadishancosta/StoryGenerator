{"cells":[{"cell_type":"markdown","source":["## Fine Tuning GPT 2 Pretrained Model\n","\n","Here the GPT 2 model is fine-tuned on a custom dataset of stories.\n","\n","These dataset is generated by combining a list of stories stored in a set location within google drive."],"metadata":{"id":"W-QqQKTeZ1zM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6pIxemS5zYHo"},"outputs":[],"source":["# The libraries that are required to run and fine-tune the GPT 2 model\n","# These libraries are provided by Hugging Face\n","!pip install -U accelerate\n","!pip install -U transformers\n","\n","# This resets the environment automatically to apply the installations above.\n","import os\n","os.kill(os.getpid(), 9)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ozol7WMxXT4_","executionInfo":{"status":"ok","timestamp":1704923620739,"user_tz":0,"elapsed":16008,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"}},"outputId":"0f0f6486-8027-479e-8033-314dfe2b9560"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5915,"status":"ok","timestamp":1704923626648,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"},"user_tz":0},"id":"qeFGhRriK5BE","outputId":"71da9a6c-41cd-4e0a-cbf5-32ba524a0bdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Device: cuda\n"]}],"source":["# Here the code checks if the system has a GPU and if so the processing is set\n","# to the GPU\n","import torch\n","\n","device = torch.device(\n","    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",")\n","print(f'Using Device: {device}')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"cmx9NkNaLcSU","executionInfo":{"status":"ok","timestamp":1704923657278,"user_tz":0,"elapsed":4434,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"}}},"outputs":[],"source":["# Here the individual story files are accessed and combined to one text file.\n","# This allows the model to be fine-tuned at once on all the data of all the\n","# selected stories.\n","import os\n","import string\n","import re\n","from google.colab import files\n","\n","data_dir = \"/content/drive/MyDrive/Colab Notebooks/Story Gen/Dataset/01 Normal\"\n","\n","# This dataset contains stories that have been repeated to match the longest\n","# story in the collection to try and eliminate bias\n","# data_dir = \"/content/drive/MyDrive/Colab Notebooks/The Story Gen/Dataset/02 Equal Length\"\n","\n","output_file = \"all_data.txt\"\n","\n","def is_hidden(filepath):\n","  return os.path.basename(filepath).startswith('.')\n","\n","with open(output_file, \"w\") as outfile:\n","  for filename in os.listdir(data_dir):\n","    filepath = os.path.join(data_dir,filename)\n","    if not is_hidden(filepath):\n","      with open(filepath,\"r\",encoding='utf-8-sig') as infile:\n","        for line in infile:\n","          if line.strip():\n","            clean = line.replace(\"\\n\", \" \").replace('. ', '.\\n')\n","            outfile.write(clean)\n","\n","# files.download('all_data.txt')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"OcDt_x8bMpHP","executionInfo":{"status":"ok","timestamp":1704923700817,"user_tz":0,"elapsed":38298,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"}}},"outputs":[],"source":["# Here the pretrained GPT-2 model and relevant Tokenizer is loaded onto memory\n","# Additionally the pad token is set to the EOS token of the GPT 2 tokenizer\n","from transformers import GPT2Tokenizer,GPT2LMHeadModel,TrainingArguments,Trainer,DataCollatorWithPadding\n","\n","from torch.utils.data import Dataset\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT2 Pretrained/tokenizer\")\n","model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT2 Pretrained/model\")\n","\n","# If a local copy of the fine-tuned model is unavailable, use the following two lines to load the model and tokenizer to load it directly from Huggingface:\n","\n","# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n","# model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n","\n","\n","if tokenizer.pad_token is None:\n","  tokenizer.pad_token = tokenizer.eos_token\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n"]},{"cell_type":"code","source":[" hempnuts\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"UMC7qu6DaF_t","executionInfo":{"status":"error","timestamp":1704924621423,"user_tz":0,"elapsed":7,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"}},"outputId":"c8a60317-72c8-4f79-a6a6-1e87c15a87a3"},"execution_count":28,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'hempnuts' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-0bc3a7b510b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhempnuts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'hempnuts' is not defined"]}]},{"cell_type":"code","execution_count":26,"metadata":{"id":"UlLGxKIpNaeW","executionInfo":{"status":"ok","timestamp":1704924606912,"user_tz":0,"elapsed":272,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"}}},"outputs":[],"source":["# This class creates the dataset using the combined text file and the GPT-2\n","# Tokenizer. The combined text file's words are converted to Tokens for\n","# processing.\n","class CustomDataset(Dataset):\n","  def __init__(self, tokenizer, file_path, block_size):\n","    self.tokenizer = tokenizer\n","    with open(file_path,\"r\") as f:\n","      self.text = f.read().splitlines()\n","\n","  def __len__(self):\n","    return len(self.text)\n","\n","  def __getitem__(self,idx):\n","    tokenized_inputs = self.tokenizer(\n","        self.text[idx],\n","        truncation = True,\n","        max_length = 128,\n","        padding = \"max_length\",\n","        return_tensors = \"pt\"\n","    )\n","    global jembus\n","    jembus = tokenized_inputs\n","    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"77LUHDryze8m","colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"status":"error","timestamp":1704924609821,"user_tz":0,"elapsed":413,"user":{"displayName":"NC proGAMer","userId":"12692661506000512665"}},"outputId":"d801aa06-1ec4-42dc-c4bf-0afeb78e2234"},"outputs":[{"output_type":"error","ename":"IndexError","evalue":"string index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-efce326d5e21>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# here as well to ensure sufficient repetitions are carried out on the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all_data.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjembus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mrented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjembus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: string index out of range"]}],"source":["# Here, the dataset, data collator and the training arguments are set.\n","# These training arguments control the how fast the model learns as well as\n","# the sizing of batches given to the model. The Epochs for training are set\n","# here as well to ensure sufficient repetitions are carried out on the dataset.\n","data = CustomDataset(tokenizer, \"all_data.txt\", 128)\n","print(CustomDataset.jembus)\n","rented = CustomDataset.jembus\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","training_args = TrainingArguments(\n","    per_device_train_batch_size = 50,\n","    num_train_epochs = 4,\n","    learning_rate = 1e-4,\n","    output_dir = './trained',\n","    load_best_model_at_end = False,\n","    evaluation_strategy = \"no\",\n","    remove_unused_columns = False,\n","    push_to_hub = False,\n","    save_total_limit = 5,\n",")"]},{"cell_type":"code","source":["# The infamous trainer.train() command passes all the training arguments\n","# tokenized dataset and data collator on for training the model.\n","# The training loss of the model can be seen in steps of 500.\n","trainer = Trainer(\n","    model=model,\n","    args = training_args,\n","    train_dataset = data,\n","    eval_dataset = None,\n","    data_collator = data_collator,\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":644},"outputId":"b95c779f-f8fd-4dc2-a183-93d229801e8f","executionInfo":{"status":"ok","timestamp":1703531817859,"user_tz":0,"elapsed":4197630,"user":{"displayName":"Mahamuge Dinendra Nadishan Costa","userId":"14270906695894466148"}},"id":"TBaCHuwYcmb-"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='8528' max='8528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8528/8528 1:09:55, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.831100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.780300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.758400</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.744300</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.655900</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.635600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.645900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.648100</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.577700</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.547900</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.542200</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.544900</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.512700</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.469700</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.470300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.470800</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.470000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=8528, training_loss=0.60569942243551, metrics={'train_runtime': 4196.9564, 'train_samples_per_second': 32.506, 'train_steps_per_second': 2.032, 'total_flos': 3.1675194588266496e+16, 'train_loss': 0.60569942243551, 'epoch': 4.0})"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2485227,"status":"ok","timestamp":1704333617567,"user":{"displayName":"Mahamuge Dinendra Costa","userId":"17358276780992758429"},"user_tz":0},"id":"0xQyRLiuzgBe","outputId":"64def95b-859c-4441-f78a-7e2704135084"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='12252' max='12252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12252/12252 3:29:06, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.689800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.607000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.548800</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.490200</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.447800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.411500</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.342400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.322700</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.306500</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.287100</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.279400</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.272000</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.233200</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.225200</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.217600</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.212200</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.209000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.201200</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.192800</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.182800</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.176000</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.171300</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.177200</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.178000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=12252, training_loss=0.3047974041930375, metrics={'train_runtime': 12547.4402, 'train_samples_per_second': 48.809, 'train_steps_per_second': 0.976, 'total_flos': 1.4219150593779302e+17, 'train_loss': 0.3047974041930375, 'epoch': 4.0})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# The infamous trainer.train() command passes all the training arguments\n","# tokenized dataset and data collator on for training the model.\n","# The training loss of the model can be seen in steps of 500.\n","# This training was on the dataset with equal length stories and so\n","# consumed over 3 hours for training (on a A 100 GPU)\n","trainer = Trainer(\n","    model=model,\n","    args = training_args,\n","    train_dataset = data,\n","    eval_dataset = None,\n","    data_collator = data_collator,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4ATg5WhRk9gY"},"outputs":[],"source":["#The below two lines of code save the fine-tuned model in Google Drive\n","model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT 2 Finetuned/model\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Generator/GPT 2 Finetuned/tokenizer\")"]},{"cell_type":"code","source":["# Here the pretrained GPT-2 model and relevant Tokenizer is loaded onto memory\n","# Additionally the pad token is set to the EOS token of the GPT 2 tokenizer\n","from transformers import GPT2Tokenizer,GPT2LMHeadModel,TrainingArguments,Trainer,DataCollatorWithPadding\n","\n","from torch.utils.data import Dataset\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n","\n","if tokenizer.pad_token is None:\n","  tokenizer.pad_token = tokenizer.eos_token\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","\n","# The below two lines of code save the pretrained model in Google Drive to\n","# easily load the model each time it is required instead of downloading from\n","# Hugging face\n","model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Gen/GPT2 Pretrained/model\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/Story Generator/GPT2 Pretrained/tokenizer\")"],"metadata":{"id":"wQaQB9o8faXW"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}